{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet git+https://github.com/m-bain/whisperx.git\n!pip install --quiet speechbrain fastapi uvicorn fastapi[all] werkzeug fastapi-cors\n# Install required libraries\n!pip install --quiet pyngrok fastapi uvicorn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nimport whisperx\nimport gc\nfrom IPython.display import display\nimport torch\nimport os\nimport subprocess\nimport datetime\nimport warnings\nfrom fastapi import FastAPI, File, Form, UploadFile\nfrom fastapi.responses import JSONResponse\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.staticfiles import StaticFiles\nfrom speechbrain.inference.speaker import SpeakerRecognition","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directories for uploads and results\nos.makedirs(\"uploads\", exist_ok=True)\nos.makedirs(\"results\", exist_ok=True)\n\napp = FastAPI()\napp.add_middleware(CORSMiddleware, allow_origins=[\"*\"], allow_methods=[\"*\"], allow_headers=[\"*\"])\napp.mount(\"/results\", StaticFiles(directory=\"results\"), name=\"results\")\n\nwarnings.filterwarnings(\"ignore\")\n\n# Initialize SpeakerRecognition\nverification = SpeakerRecognition.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configurations\nWHISPER_MODEL = \"large-v2\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nbatch_size = 4\ncompute_type = \"float32\" if torch.cuda.is_available() else \"int8\"\nhf_token = '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'\n\nmodel = whisperx.load_model(WHISPER_MODEL, device=DEVICE, compute_type=compute_type, language=\"en\")\ndiarize_model = whisperx.DiarizationPipeline(use_auth_token=hf_token, device=DEVICE)\n\n# Known speaker embeddings\nknown_speakers = {}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Utility functions\ndef ensure_wav_format(file_path):\n    wav_path = os.path.splitext(file_path)[0] + \".wav\"\n    subprocess.call([\"ffmpeg\", \"-i\", file_path, wav_path, \"-y\"])\n    return wav_path\n\ndef get_file_embedding(file_path):\n    signal = verification.load_audio(file_path)\n    embedding = verification.encode_batch(signal)\n    return embedding.squeeze().cpu().numpy()\n\ndef time_format(secs):\n    return str(datetime.timedelta(seconds=round(secs)))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydantic import BaseModel\nfrom typing import List\nfrom fastapi import UploadFile, Form, File\n\nclass EnrollRequest(BaseModel):\n    num_speakers: int\n    names: List[str]\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from fastapi import FastAPI, UploadFile, File, Form, HTTPException, Depends\n\n@app.post(\"/enroll\")\nasync def enroll_speakers(\n    num_speakers: int = Form(...),\n    names: List[str] = Form(...),\n    files: List[UploadFile] = File(...),\n):\n    if len(names) != num_speakers:\n        raise HTTPException(status_code=400, detail=\"Number of names doesn't match number of speakers\")\n\n    if len(files) != num_speakers:\n        raise HTTPException(status_code=400, detail=\"Number of audio files doesn't match number of speakers\")\n\n    for i, name in enumerate(names):\n        audio_file = files[i]\n        filepath = os.path.join('uploads', audio_file.filename)\n        with open(filepath, \"wb\") as buffer:\n            buffer.write(await audio_file.read())\n        \n        wav_path = ensure_wav_format(filepath)\n        embedding = get_file_embedding(wav_path)\n        known_speakers[name] = embedding\n\n    return {\n        \"message\": \"Speakers enrolled successfully\",\n        \"enrolled_speakers\": list(known_speakers.keys())\n    }\n    \n@app.post(\"/transcribe\")\nasync def transcribe_conversation(audio: UploadFile = File(...)):\n    filepath = os.path.join(\"uploads\", audio.filename)\n    with open(filepath, \"wb\") as f:\n        f.write(audio.file.read())\n    wav_path = ensure_wav_format(filepath)\n    \n    # Transcribe audio\n    audio_data = whisperx.load_audio(wav_path)\n    result1 = model.transcribe(audio_data)\n    model_a, metadata = whisperx.load_align_model(language_code=result1[\"language\"], device=DEVICE)\n    result2 = whisperx.align(result1[\"segments\"], model_a, metadata, audio_data, DEVICE, return_char_alignments=False)\n    \n    # Diarization\n    diarize_segments = diarize_model(audio_data, min_speakers=len(known_speakers))\n    result = whisperx.assign_word_speakers(diarize_segments, result2)\n    \n    # Speaker matching with detailed accuracy\n    speaker_mapping = {}\n    speaker_accuracies = {}\n    available_known_speakers = list(known_speakers.keys())\n    \n    for diarized_speaker in diarize_segments.speaker.unique():\n        speaker_segments = diarize_segments[diarize_segments.speaker == diarized_speaker]\n        segment_audio = audio_data[int(speaker_segments.start.iloc[0] * 16000):int(speaker_segments.end.iloc[-1] * 16000)]\n        \n        # Verify speaker embedding\n        diarized_embedding = verification.encode_batch(torch.tensor(segment_audio).unsqueeze(0)).squeeze().cpu().numpy()\n        \n        # Compute similarity scores with detailed tracking\n        scores = {}\n        for name in available_known_speakers:\n            similarity = verification.similarity(\n                torch.tensor(diarized_embedding).unsqueeze(0), \n                torch.tensor(known_speakers[name]).unsqueeze(0)\n            ).item()\n            scores[name] = {\n                'similarity_score': similarity,\n                'confidence_percentage': round(similarity * 100, 2)\n            }\n        \n        # Find best match\n        best_match = max(scores, key=lambda k: scores[k]['similarity_score'])\n        speaker_mapping[diarized_speaker] = best_match\n        speaker_accuracies[diarized_speaker] = scores[best_match]\n        \n        available_known_speakers.remove(best_match)\n        if not available_known_speakers:\n            break\n    \n    # Assign unnamed speakers\n    for diarized_speaker in diarize_segments.speaker.unique():\n        if diarized_speaker not in speaker_mapping:\n            unnamed_speaker = f\"Unknown_Speaker_{len(speaker_mapping) + 1}\"\n            speaker_mapping[diarized_speaker] = unnamed_speaker\n            speaker_accuracies[diarized_speaker] = {\n                'similarity_score': 0,\n                'confidence_percentage': 0\n            }\n    \n    # Enhance segments with accuracy information\n    for segment in result[\"segments\"]:\n        # Add speaker information\n        speaker = speaker_mapping.get(segment.get(\"speaker\", \"\"), \"Unknown\")\n        segment[\"speaker\"] = speaker\n        \n        # Add transcription confidence\n        segment[\"transcription_confidence\"] = round(segment.get(\"confidence\", 0) * 100, 2)\n        \n        # Add speaker verification accuracy\n        segment[\"speaker_confidence\"] = speaker_accuracies.get(\n            segment.get(\"speaker\", \"\"), \n            {'confidence_percentage': 0}\n        )['confidence_percentage']\n    \n    # Save transcription with enhanced details\n    transcription_path = os.path.join(\"results\", f\"transcription_{audio.filename}.txt\")\n    with open(transcription_path, \"w\") as f:\n        for segment in result[\"segments\"]:\n            f.write(\n                f\"[{time_format(segment['start'])} - {time_format(segment['end'])}] \"\n                f\"{segment['speaker']} (Speaker Conf: {segment['speaker_confidence']}%, \"\n                f\"Transcription Conf: {segment['transcription_confidence']}%): \"\n                f\"{segment['text']}\\n\"\n            )\n    \n    # Read the transcription file content\n    with open(transcription_path, \"r\") as f:\n        transcript_data = f.read()\n    \n    return {\n        \"message\": \"Transcription completed\", \n        \"transcription_file\": f\"/results/transcription_{audio.filename}.txt\", \n        \"transcription\": transcript_data,\n        \"overall_accuracy\": {\n            \"language_detected\": result1[\"language\"],\n            \"speaker_verification_details\": speaker_accuracies\n        }\n    }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pyngrok import ngrok\nimport uvicorn\nimport threading\n\n# Start ngrok tunnel\nNGROK_AUTH_TOKEN = \"2pPDOOnxTKaVSPdfEguG5o85TJp_4SDRE9oFkFuRxiyinexzt\"  # Replace with your ngrok auth token\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\npublic_url = ngrok.connect(8000).public_url\nprint(f\"Public URL: {public_url}\")\n\n# Function to run FastAPI server\n# Run the server directly\ndef run_server():\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=False)\n\n# Start FastAPI server in a separate thread\nserver_thread = threading.Thread(target=run_server, daemon=True)\nserver_thread.start()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}